import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix 
from plotcm import plot_confusion_matrix
import confusion_matrix_self
import torch
import torch.optim as optim
import torch.nn as nn 
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms

import tensorflow as tf




class MyNetwork(nn.Module):
	def __init__(self):
		super(MyNetwork, self).__init__()
		self.conv1 = nn.Conv2d(3, 32, 5 ,padding=(2,2))
		self.maxpool = nn.MaxPool2d(2, 2)
		self.avgpool=nn.AvgPool2d(2,2)
		self.conv2 = nn.Conv2d(32, 32, 5 ,padding=(2,2))
		self.conv3=nn.Conv2d(32,64, 5 ,padding=(2,2))
		self.conv4=nn.Conv2d(64,64,4)
		self.fc1 = nn.Linear(64, 10)
		#self.fc2=nn.Linear(128,64)
		#self.fc3=nn.Linear(64,10)
		


	def forward(self, x):
		x = F.relu(self.maxpool(self.conv1(x)))
		x = self.avgpool(F.relu(self.conv2(x)))
		x = self.avgpool(F.relu(self.conv3(x)))
		x=F.relu(self.conv4(x))
		x = x.view(-1,64)
		x = self.fc1(x)
		#x = F.softmax(x,dim=1)

		#print(x.size())
		
		return x
	
class MyNetwork_Dropout(nn.Module):
	def __init__(self):
		super(MyNetwork_Dropout, self).__init__()
		self.conv1 = nn.Conv2d(3, 32, 5 ,padding=(2,2))
		self.maxpool = nn.MaxPool2d(2, 2)
		self.avgpool = nn.AvgPool2d(2,2)
		self.conv2 = nn.Conv2d(32, 32, 5 ,padding=(2,2))
		self.conv3 = nn.Conv2d(32,64, 5 ,padding=(2,2))
		self.conv4 = nn.Conv2d(64,64,4)
		self.fc1 = nn.Linear(64, 10)
		#self.fc2=nn.Linear(128,64)
		#self.fc3=nn.Linear(64,10)
		self.dropout2 = nn.Dropout(0.15)
		self.dropout1 = nn.Dropout(0.35)
		self.dropout1 = nn.Dropout(0.35)


	def forward(self, x):
		#print(x.size())
		x = self.dropout1(x)
		x = F.relu(self.maxpool(self.conv1(x)))
		x = self.dropout1(x)
		x = self.avgpool(F.relu(self.conv2(x)))
		x = self.dropout1(x)
		x = self.avgpool(F.relu(self.conv3(x)))
		x = self.dropout2(x)
		x = F.relu(self.conv4(x))
		
		x = x.view(-1,64)
		x = self.fc1(x)

		
		return x

class NetworkTrainer():

	def __init__(self,net,print_freq=250,batch_size=100,epochs=30):

		self.transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
		self.batch_size=batch_size
		self.trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=self.transform)
		self.trainloader = torch.utils.data.DataLoader(self.trainset, batch_size=self.batch_size, shuffle=True, num_workers=0)
		
		self.testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=self.transform)
		self.testloader = torch.utils.data.DataLoader(self.testset, batch_size=self.batch_size,shuffle=False, num_workers=0)


		self.classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')

		self.epochs=epochs
		self.dataiter = iter(self.trainloader)
		self.images, self.labels = self.dataiter.next()
		self.criterion = nn.CrossEntropyLoss()
		self.optimizer1 = optim.SGD(net.parameters(), lr=0.05)#, momentum=0.5)
		self.optimizer2 = optim.SGD(net.parameters(), lr=0.005)#, momentum=0.5)
		self.optimizer3 = optim.SGD(net.parameters(), lr=0.0005)#, momentum=0.5)
		self.model = net
		self.print_freq = print_freq
		
		
		self.train_loss_vec = []
		self.val_loss_vec = []
		self.gradient_abs_vec = []
		self.gradient_average_abs_vec = []
		self.top1error_test_vec = []
		self.top1error_train_vec = []
		self.top5error_test_vec = []
		self.top5error_train_vec = []
		
		self.gradient_counter = []
		self.epoch_counter_train = 0
		self.iter_counter_train = 0
		self.counter_train = []
		self.counter_val = []
		
		self.train()


	def train(self):
		for epoch in range(self.epochs):  # loop over the dataset multiple times
			self.epoch_counter_train += 1
			if epoch>=0:
				optimizer=self.optimizer1
			if epoch>19:
				optimizer=self.optimizer2
			if epoch>24:
				optimizer=self.optimizer3
			
			correct = 0
			total = 0
			running_loss = 0.0
			correct_top5 = 0

			for i, data in enumerate(self.trainloader, 0):
				self.iter_counter_train += 1
				# get the inputs; data is a list of [inputs, labels]
				inputs, labels = data

				# zero the parameter gradients
				optimizer.zero_grad()

				# forward + backward + optimize
				outputs = self.model(inputs)
				loss = self.criterion(outputs, labels)
				loss.backward()
				optimizer.step()
				
				# print statistics
				running_loss += loss.item()
				
				
				_, predicted = torch.max(outputs.data, 1)

				total += labels.size(0)
				correct += (predicted == labels).sum().item()

				
				values,indices = torch.topk(outputs,k=5,dim=1,sorted=False)
				indices = indices.t()
				#print(correct_top5)
				correct_top5 += (indices == labels.view(1, -1).expand_as(indices)).sum().item()
				


				
				if i % self.print_freq == self.print_freq-1:    # print every print_freq mini-batches
					print('[%d, %5d] training loss: %.3f' %(epoch + 1, i + 1, running_loss/self.print_freq))
					
					#eval_loss,top1error,top5error = self.evaluate(epoch,i,self.testloader)
					top1error = 1-correct/total
					top5error = 1-correct_top5/total
					self.top1error_train_vec.append(top1error)
					self.top5error_train_vec.append(top5error)


					self.evaluate(epoch,i)
					
					self.train_loss_vec.append(running_loss/self.print_freq)
					self.counter_train.append(self.iter_counter_train/len(self.trainloader))
					
					running_loss = 0.0


		
		train_loss_plot = np.asarray(self.train_loss_vec)
		train_loss_axis = np.asarray(self.counter_train)
		
		val_loss_plot = np.asarray(self.val_loss_vec)
		val_loss_axis = np.asarray(self.counter_val)
		
		top1error_test_plot = np.asarray(self.top1error_test_vec)
		top5error_test_plot = np.asarray(self.top5error_test_vec)
		
		top1error_train_plot = np.asarray(self.top1error_train_vec)
		top5error_train_plot = np.asarray(self.top5error_train_vec)
		
		plt.figure()

		plt.plot(train_loss_axis, train_loss_plot, c='red', label='training loss')
		plt.plot(val_loss_axis, val_loss_plot, c='green', label='validation loss')
		plt.yscale('linear')
		plt.xlabel('epochs')
		plt.ylabel('CrossEntropyLoss')
		plt.title('Training and Validation loss with Dropout')
		plt.grid()
		plt.legend()
		plt.show()
		plt.savefig('Eval' + '/training.pdf')


		plt.figure()
		plt.plot(val_loss_axis, top1error_test_plot, c='green', label='Top1Error validation')
		plt.plot(val_loss_axis, top1error_train_plot, c='red',label='Top1Error training' )

		plt.yscale('linear')
		plt.xlabel('epochs')
		plt.ylabel('percentage')
		plt.title('Top-1-Error with Dropout')
		plt.grid()
		plt.legend()
		plt.show()
		plt.savefig('Top1Error' + '/training.pdf')

		plt.figure()
		plt.plot(val_loss_axis, top5error_test_plot, c='green',label='Top5Error validation ' )
		plt.plot(val_loss_axis, top5error_train_plot, c='red', label='Top5Error training ')

		plt.yscale('linear')
		plt.xlabel('epochs')
		plt.ylabel('percentage')
		plt.title('Top-5-Error with Dropout')
		plt.grid()
		plt.legend()
		plt.show()
		plt.savefig('Top5Error' + '/training.pdf')


		self.results()


	def evaluate(self,epoch,i):

		self.model.eval()
		correct = 0
		total = 0
		total_loss = 0
		correct_top5 = 0
		# turn off gradients for validation
		with torch.no_grad():
			for data in self.testloader:
				image,labels=data
				# forward pass
				outputs = self.model(image)
				# validation batch loss
				loss = self.criterion(outputs, labels) 
				# accumulate the valid_loss
				total_loss += loss.item()

				values,indices = torch.topk(outputs,k=5,dim=1,sorted=False)
				indices = indices.t()
				correct_top5 += (indices == labels.view(1, -1).expand_as(indices)).sum().item()

				_, predicted = torch.max(outputs.data, 1)

				total += labels.size(0)
				correct += (predicted == labels).sum().item()



			top1error = 1-correct/total
			top5error = 1-correct_top5/total

			total_loss/=len(self.testloader)


		self.counter_val.append(self.iter_counter_train/len(self.trainloader))



		self.top1error_test_vec.append(top1error)
		self.top5error_test_vec.append(top5error)
		self.val_loss_vec.append(total_loss)

		print('[%d, %5d] validation loss: %.3f' %(epoch + 1, i + 1, total_loss))



	def results(self):
		correct = 0
		total = 0
		class_correct = list(0. for i in range(10))
		class_total = list(0. for i in range(10))

		with torch.no_grad():
			all_preds = torch.tensor([])
			all_labels = torch.tensor([])


			for data in self.testloader:
				images, labels = data
				outputs = self.model(images)
				#print(labels.type())


				_, predicted = torch.max(outputs.data, 1)

				all_preds = torch.cat((all_preds, outputs.float()), dim=0)
				all_labels=torch.cat((all_labels,labels.float()),dim=0)


				total += labels.size(0)
				correct += (predicted == labels).sum().item()

				_, predicted = torch.max(outputs, 1)
				c = (predicted == labels).squeeze()
				
				for i in range(self.batch_size):
					label = labels[i]
					class_correct[label] += c[i].item()
					class_total[label] += 1



		confusion_matrix_self._test_data_class(all_labels,all_preds.argmax(dim=1),self.classes)

		print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))
		for i in range(10):
			print('Accuracy of %5s : %2d %%' % (
			self.classes[i], 100 * class_correct[i] / class_total[i]))




net_dropout = MyNetwork_Dropout()
net = MyNetwork()

NetworkTrainer(net_drop#)
NetworkTrainer(net)





